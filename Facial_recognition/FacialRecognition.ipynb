{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\coalworks\\.conda\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (72.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\coalworks\\.conda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in d:\\coalworks\\.conda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.0)\n",
      "Requirement already satisfied: namex in d:\\coalworks\\.conda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\coalworks\\.conda\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\coalworks\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\coalworks\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\coalworks\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\coalworks\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\coalworks\\.conda\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\coalworks\\.conda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\coalworks\\.conda\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\coalworks\\.conda\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\coalworks\\.conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in d:\\coalworks\\.conda\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\coalworks\\.conda\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.0-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.53.1-cp311-cp311-win_amd64.whl.metadata (165 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\coalworks\\.conda\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\coalworks\\.conda\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\coalworks\\.conda\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\coalworks\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp311-cp311-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/7.8 MB 4.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.8/7.8 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.9/7.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/7.8 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.0/7.8 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.8 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.0-cp311-cp311-win_amd64.whl (217 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.53.1-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-win_amd64.whl (56 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.2 pillow-10.4.0 pyparsing-3.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow \n",
    "%pip install opencv-python \n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_PATH = os.path.join('images', 'true_images')\n",
    "FALSE_PATH = os.path.join('images', 'false_images')\n",
    "INPUT_PATH = os.path.join('images', 'input_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('lfw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'lfw'\n",
    "for folder in os.listdir(path):\n",
    "    for img_file in os.listdir(os.path.join(path,folder)):\n",
    "\n",
    "        CURRENT_PATH= os.path.join(path, folder, img_file)\n",
    "        NEW_PATH = os.path.join(FALSE_PATH, img_file)\n",
    "        os.replace(CURRENT_PATH, NEW_PATH)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13233"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(FALSE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images\\\\true_images\\\\b7f8f8e0-660d-11ef-84db-08bfb8012a40.jpg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(TRUE_PATH, f\"{uuid.uuid1()}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap =  cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    frame = cv2.resize(frame,(250,250))\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF== ord('i'):\n",
    "        print(\"ready for click! i\")\n",
    "        path = os.path.join(INPUT_PATH,f\"{uuid.uuid1()}.jpg\")\n",
    "        cv2.imwrite(path,frame)\n",
    "        print('clicked i')\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF== ord('t'):\n",
    "        print(\"ready for click t!\")\n",
    "        path = os.path.join(TRUE_PATH,f\"{uuid.uuid1()}.jpg\")\n",
    "        cv2.imwrite(path,frame)\n",
    "        print('clicked t')\n",
    "        \n",
    "        # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m true_img \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(TRUE_PATH,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1dec09d8-3337-11ed-8605-767b4db31ca2.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(true_img)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "true_img = os.path.join(TRUE_PATH,\"1dec09d8-3337-11ed-8605-767b4db31ca2.jpg\")\n",
    "img = cv2.imread(true_img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img):\n",
    "  data = []\n",
    "  for i in range(9):\n",
    "    img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "    img = tf.image.stateless_random_contrast(img, lower=0.6,upper= 1, seed=(1,3))\n",
    "    img = tf.image.stateless_random_flip_left_right(img, seed = (np.random.randint(100),np.random.randint(100)))\n",
    "    img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90,max_jpeg_quality=100, seed = (np.random.randint(100),np.random.randint(100)))\n",
    "    img = tf.image.stateless_random_saturation(img, lower=0.9, upper=1,seed = (np.random.randint(100),np.random.randint(100)))\n",
    "\n",
    "    data.append(img)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(os.path.join(INPUT_PATH)):\n",
    "    img_path = os.path.join(INPUT_PATH, file_name)\n",
    "    print(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join('images','input_images','0605c302-3337-11ed-ae4c-767b4db31ca2.jpg')\n",
    "img = cv2.imread(img_path)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_imgs = data_aug(img)\n",
    "aug_imgs\n",
    "for image in aug_imgs:\n",
    "    plt.imshow(image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input images augumentation\n",
    "for file_name in os.listdir(os.path.join(INPUT_PATH)):\n",
    "    img_path = os.path.join(INPUT_PATH, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    aug_imgs = data_aug(img)\n",
    "\n",
    "for image in aug_imgs:\n",
    "    cv2.imwrite(os.path.join(INPUT_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#true images augumentation\n",
    "for file_name in os.listdir(os.path.join(TRUE_PATH)):\n",
    "    img_path = os.path.join(TRUE_PATH, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    aug_imgs = data_aug(img)\n",
    "\n",
    "for image in aug_imgs:\n",
    "    cv2.imwrite(os.path.join(TRUE_PATH, '{}.jpg'.format(uuid.uuid1())), image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fileset = tf.data.Dataset.list_files(INPUT_PATH+'/*.jpg').take(20)\n",
    "true_fileset= tf.data.Dataset.list_files(TRUE_PATH+'/*.jpg').take(20)\n",
    "false_fileset = tf.data.Dataset.list_files(FALSE_PATH+'/*.jpg').take(20)\n",
    "\n",
    "input_fileset = tf.data.Dataset.list_files(INPUT_PATH+'/*.jpg').take(2000)\n",
    "true_fileset= tf.data.Dataset.list_files(TRUE_PATH+'/*.jpg').take(2000)\n",
    "false_fileset = tf.data.Dataset.list_files(FALSE_PATH+'/*.jpg').take(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fileset,true_fileset,false_fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test = input_fileset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_imgfiles_and_preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path( retrun the content of file as tensor)\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    \n",
    "    # Load in the image (Decode a JPEG-encoded image to a uint8 tensor.)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 105x105x3\n",
    "    img = tf.image.resize(img, (105,105))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "    \n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trueset = tf.data.Dataset.zip((input_fileset, true_fileset, tf.data.Dataset.from_tensor_slices(tf.ones(len(input_fileset)))))\n",
    "falseset = tf.data.Dataset.zip((input_fileset, false_fileset, tf.data.Dataset.from_tensor_slices(tf.zeros(len(input_fileset)))))\n",
    "dataset = trueset.concatenate(falseset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(input_file, validation_file, label):\n",
    "    return(access_imgfiles_and_preprocess(input_file), access_imgfiles_and_preprocess(validation_file), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloader pipeline\n",
    "dataset = dataset.map(preprocess_dataset)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = dataset.take(round(len(dataset)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = dataset.skip(round(len(dataset)*.7))\n",
    "test_data = test_data.take(round(len(dataset)*.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(Model):\n",
    "    def train_step(self, batch):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        #x, y = data\n",
    "        x = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_initial_model_layers(): \n",
    "    inp = Input(shape=(105,105,3), name='input_image')\n",
    "    \n",
    "    # First block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "    \n",
    "    # Second block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    # Third block \n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "    \n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    \n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "    \n",
    "    \n",
    "    return CustomModel(inputs=[inp], outputs=[d1], name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = build_initial_model_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese L1 Distance class\n",
    "class DistanceLayer(Layer):\n",
    "    \n",
    "    # Init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "       \n",
    "    # Magic happens here - similarity calculation\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model(): \n",
    "    \n",
    "    # Anchor image input in the network\n",
    "    input_image = Input(name='input_img', shape=(105,105,3))\n",
    "    \n",
    "    # Validation image in the network \n",
    "    validation_image = Input(name='validation_img', shape=(105,105,3))\n",
    "    \n",
    "    embedding = build_initial_model_layers()\n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = DistanceLayer()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    # Classification layer \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return CustomModel(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cross_loss = tf.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(1e-4) # 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint('frmoidel.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=binary_cross_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, epochs=100, checkpoint = [checkpointer])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
